{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434c390b",
   "metadata": {},
   "source": [
    "# Stage 03: Albert & DeBerta (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "ROOT = Path(\".\").resolve()  # run from project root\n",
    "TRAIN_PATH = ROOT / \"data\" / \"processed\" / \"pcl_task1_train.csv\"\n",
    "DEV_PATH   = ROOT / \"data\" / \"processed\" / \"pcl_task1_dev.csv\"\n",
    "\n",
    "OUTPUT_DIR = ROOT / \"runs\" / \"optuna_task1\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STUDY_DB = str(OUTPUT_DIR / \"optuna_pcl_task1.db\")  # sqlite db file\n",
    "STUDY_NAME = \"pcl_task1_binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc717aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df   = pd.read_csv(DEV_PATH)\n",
    "\n",
    "# Keep only what we need\n",
    "keep_cols = [\"par_id\", \"text\", \"label_bin\"]\n",
    "train_df = train_df[keep_cols].copy()\n",
    "dev_df   = dev_df[keep_cols].copy()\n",
    "\n",
    "train_df[\"label_bin\"] = train_df[\"label_bin\"].astype(int)\n",
    "dev_df[\"label_bin\"]   = dev_df[\"label_bin\"].astype(int)\n",
    "\n",
    "print(train_df.shape, dev_df.shape)\n",
    "print(train_df[\"label_bin\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8616fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.15,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df[\"label_bin\"],\n",
    ")\n",
    "\n",
    "print(\"train:\", train_split[\"label_bin\"].value_counts().to_dict())\n",
    "print(\"val  :\", val_split[\"label_bin\"].value_counts().to_dict())\n",
    "\n",
    "ds_train_raw = Dataset.from_pandas(train_split.reset_index(drop=True))\n",
    "ds_val_raw   = Dataset.from_pandas(val_split.reset_index(drop=True))\n",
    "ds_dev_raw   = Dataset.from_pandas(dev_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_from_logits(logits, labels, threshold=0.5):\n",
    "    # logits: (N,1) or (N,)\n",
    "    logits = np.squeeze(logits)\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    y = labels.astype(int)\n",
    "\n",
    "    f1  = f1_score(y, preds, zero_division=0)\n",
    "    p   = precision_score(y, preds, zero_division=0)\n",
    "    r   = recall_score(y, preds, zero_division=0)\n",
    "    acc = accuracy_score(y, preds)\n",
    "\n",
    "    cm = confusion_matrix(y, preds, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc0 = tn / (tn + fp + 1e-12)\n",
    "    acc1 = tp / (tp + fn + 1e-12)\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1,\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"accuracy\": acc,\n",
    "        \"acc_nonpcl\": acc0,\n",
    "        \"acc_pcl\": acc1,\n",
    "        \"tp\": int(tp), \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn),\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    return compute_metrics_from_logits(logits, labels, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenized_datasets(model_name: str, max_length: int):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    def tok_fn(batch):\n",
    "        return tok(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    ds_train = ds_train_raw.map(tok_fn, batched=True)\n",
    "    ds_val   = ds_val_raw.map(tok_fn, batched=True)\n",
    "    ds_dev   = ds_dev_raw.map(tok_fn, batched=True)\n",
    "\n",
    "    cols_to_keep = [\"input_ids\", \"attention_mask\", \"label_bin\"]\n",
    "    ds_train = ds_train.rename_column(\"label_bin\", \"labels\").with_columns({\n",
    "        \"labels\": ds_train[\"labels\"]\n",
    "    }).remove_columns([c for c in ds_train.column_names if c not in cols_to_keep and c != \"labels\"])\n",
    "\n",
    "    ds_val = ds_val.rename_column(\"label_bin\", \"labels\").remove_columns(\n",
    "        [c for c in ds_val.column_names if c not in [\"input_ids\",\"attention_mask\",\"labels\"]]\n",
    "    )\n",
    "    ds_dev = ds_dev.rename_column(\"label_bin\", \"labels\").remove_columns(\n",
    "        [c for c in ds_dev.column_names if c not in [\"input_ids\",\"attention_mask\",\"labels\"]]\n",
    "    )\n",
    "\n",
    "    return tok, ds_train, ds_val, ds_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062f284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBCETrainer(Trainer):\n",
    "    def __init__(self, *args, pos_weight=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").float()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.squeeze(-1)  # (B,)\n",
    "\n",
    "        # BCEWithLogits with pos_weight\n",
    "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    model_name = trial.suggest_categorical(\n",
    "        \"model_name\",\n",
    "        [\n",
    "            \"microsoft/deberta-v3-base\",\n",
    "            \"microsoft/deberta-v3-small\",\n",
    "            \"albert-base-v2\",\n",
    "            \"albert-large-v2\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    lr          = trial.suggest_float(\"lr\", 5e-6, 5e-5, log=True)\n",
    "    batch_size  = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    weight_decay= trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    warmup_ratio= trial.suggest_float(\"warmup_ratio\", 0.0, 0.15)\n",
    "    max_length  = trial.suggest_categorical(\"max_length\", [96, 128, 192, 256])\n",
    "    epochs      = trial.suggest_categorical(\"epochs\", [2, 3, 4])\n",
    "    grad_accum  = trial.suggest_categorical(\"grad_accum\", [1, 2, 4])\n",
    "    pos_scale   = trial.suggest_categorical(\"pos_weight_scale\", [0.75, 1.0, 1.25, 1.5, 2.0])\n",
    "\n",
    "    tok, ds_train, ds_val, _ = make_tokenized_datasets(model_name, max_length)\n",
    "\n",
    "    # base pos_weight = neg/pos\n",
    "    y = np.array(train_split[\"label_bin\"].values, dtype=int)\n",
    "    n_pos = (y == 1).sum()\n",
    "    n_neg = (y == 0).sum()\n",
    "    base_pos_weight = (n_neg / max(n_pos, 1))\n",
    "    pos_weight = torch.tensor(base_pos_weight * pos_scale, dtype=torch.float)\n",
    "\n",
    "    cfg = AutoConfig.from_pretrained(model_name)\n",
    "    cfg.num_labels = 1  # single logit\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=cfg)\n",
    "\n",
    "    trial_dir = OUTPUT_DIR / f\"trial_{trial.number:04d}\"\n",
    "    trial_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(trial_dir),\n",
    "        seed=SEED,\n",
    "        data_seed=SEED,\n",
    "\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=grad_accum,\n",
    "\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "\n",
    "        save_total_limit=1,\n",
    "\n",
    "        fp16=torch.cuda.is_available(),   # safe; ignored on MPS/CPU\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = WeightedBCETrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        tokenizer=tok,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    "        pos_weight=pos_weight,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    metrics = trainer.evaluate(ds_val)\n",
    "    # Optuna optimizes this:\n",
    "    return metrics[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a535426",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_url = f\"sqlite:///{STUDY_DB}\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    direction=\"maximize\",\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True,\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=0),\n",
    ")\n",
    "\n",
    "print(\"Existing trials:\", len(study.trials))\n",
    "\n",
    "# tweakable:\n",
    "N_TRIALS = 20\n",
    "\n",
    "study.optimize(objective, n_trials=N_TRIALS, gc_after_trial=True)\n",
    "\n",
    "print(\"Best f1:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = study.best_params\n",
    "best_model = best[\"model_name\"]\n",
    "best_maxlen = best[\"max_length\"]\n",
    "\n",
    "# rebuild datasets: train on (train+val), evaluate on dev\n",
    "full_train = pd.concat([train_split, val_split], ignore_index=True)\n",
    "ds_full_train_raw = Dataset.from_pandas(full_train.reset_index(drop=True))\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(best_model, use_fast=True)\n",
    "def tok_fn(batch):\n",
    "    return tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=best_maxlen)\n",
    "\n",
    "ds_full_train = ds_full_train_raw.map(tok_fn, batched=True)\n",
    "ds_dev = ds_dev_raw.map(tok_fn, batched=True)\n",
    "\n",
    "ds_full_train = ds_full_train.rename_column(\"label_bin\",\"labels\").remove_columns(\n",
    "    [c for c in ds_full_train.column_names if c not in [\"input_ids\",\"attention_mask\",\"labels\"]]\n",
    ")\n",
    "ds_dev = ds_dev.rename_column(\"label_bin\",\"labels\").remove_columns(\n",
    "    [c for c in ds_dev.column_names if c not in [\"input_ids\",\"attention_mask\",\"labels\"]]\n",
    ")\n",
    "\n",
    "y_full = np.array(full_train[\"label_bin\"].values, dtype=int)\n",
    "n_pos = (y_full == 1).sum()\n",
    "n_neg = (y_full == 0).sum()\n",
    "base_pos_weight = (n_neg / max(n_pos, 1))\n",
    "pos_weight = torch.tensor(base_pos_weight * best[\"pos_weight_scale\"], dtype=torch.float)\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(best_model)\n",
    "cfg.num_labels = 1\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_model, config=cfg)\n",
    "\n",
    "final_dir = OUTPUT_DIR / \"best_final_model\"\n",
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(final_dir),\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "\n",
    "    learning_rate=best[\"lr\"],\n",
    "    per_device_train_batch_size=best[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best[\"batch_size\"],\n",
    "    gradient_accumulation_steps=best[\"grad_accum\"],\n",
    "\n",
    "    num_train_epochs=best[\"epochs\"],\n",
    "    weight_decay=best[\"weight_decay\"],\n",
    "    warmup_ratio=best[\"warmup_ratio\"],\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = WeightedBCETrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_full_train,\n",
    "    eval_dataset=ds_dev,\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    "    pos_weight=pos_weight,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "dev_metrics = trainer.evaluate(ds_dev)\n",
    "dev_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
