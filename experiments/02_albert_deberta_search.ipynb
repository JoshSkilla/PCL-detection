{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434c390b",
   "metadata": {},
   "source": [
    "# Stage 03: Albert & DeBerta Grid Search (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc83903",
   "metadata": {},
   "source": [
    "## Imports Load & Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f912ec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshua_killa/.pyenv/versions/pcl-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8375, 3) (2094, 3)\n",
      "label_bin\n",
      "0    7581\n",
      "1     794\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "from src.training.metrics import compute_metrics, compute_metrics_from_logits\n",
    "from src.training.tokenization_utils import make_tokenized_datasets, clean_and_prune_by_tokens\n",
    "from src.training.loss import WeightedBCETrainer\n",
    "from src.training.search_utils import (\n",
    "    best_so_far_df, \n",
    "    progress_df, \n",
    "    reset_study_completely, \n",
    "    clean_trial_folders, \n",
    "    mark_stale_running_trials_as_fail, \n",
    "    remaining_trials_to_run, \n",
    "    done_counts,\n",
    "    OptunaMedianPruningCallback,\n",
    "    pretty_print_dict,\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "ROOT = Path('.').resolve().parents[0]  # run from project root\n",
    "TRAIN_PATH = ROOT / \"data\" / \"processed\" / \"pcl_task1_train.csv\"\n",
    "DEV_PATH   = ROOT / \"data\" / \"processed\" / \"pcl_task1_dev.csv\"\n",
    "\n",
    "OUTPUT_DIR = ROOT / \"runs\" / \"optuna_task1\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STUDY_DB = str(OUTPUT_DIR / \"optuna_pcl_task1.db\")  # sqlite db file\n",
    "STUDY_NAME = \"pcl_task1_binary\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df   = pd.read_csv(DEV_PATH)\n",
    "\n",
    "# Keep only what we need\n",
    "keep_cols = [\"par_id\", \"text\", \"label_bin\"]\n",
    "train_df = train_df[keep_cols].copy()\n",
    "dev_df   = dev_df[keep_cols].copy()\n",
    "\n",
    "train_df[\"label_bin\"] = train_df[\"label_bin\"].astype(int)\n",
    "dev_df[\"label_bin\"]   = dev_df[\"label_bin\"].astype(int)\n",
    "\n",
    "print(train_df.shape, dev_df.shape)\n",
    "print(train_df[\"label_bin\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94269fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"No CUDA GPU detected. Using CPU or MPS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd068b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_MODEL_NAME = \"microsoft/deberta-v3-base\"  # one of the two models used for tokenising\n",
    "CONTEXT_WINDOW = 128  # max tokens to keep (after tokenization)\n",
    "train_df = clean_and_prune_by_tokens(train_df, EXAMPLE_MODEL_NAME, max_pos_tokens=CONTEXT_WINDOW*1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967534e",
   "metadata": {},
   "source": [
    "## Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e8616fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {0: 6444, 1: 671}\n",
      "val  : {0: 1137, 1: 119}\n"
     ]
    }
   ],
   "source": [
    "train_split, val_split = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.15,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df[\"label_bin\"], # Keep balance of classes\n",
    ")\n",
    "\n",
    "print(\"train:\", train_split[\"label_bin\"].value_counts().to_dict())\n",
    "print(\"val  :\", val_split[\"label_bin\"].value_counts().to_dict())\n",
    "\n",
    "ds_train_raw = Dataset.from_pandas(train_split.reset_index(drop=True))\n",
    "ds_val_raw   = Dataset.from_pandas(val_split.reset_index(drop=True))\n",
    "ds_dev_raw   = Dataset.from_pandas(dev_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13d2c4",
   "metadata": {},
   "source": [
    "## Define search space for configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba913f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters\n",
    "fixed_maxlen = CONTEXT_WINDOW  # 75% fit under 67 tokens    # trial.suggest_categorical(\"max_length\", [96, 128, 192, 256])\n",
    "fixed_batch_size = 16\n",
    "fixed_epochs = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import trial\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    try:\n",
    "        model_name = trial.suggest_categorical(\n",
    "            \"model_name\",\n",
    "            [\n",
    "                \"microsoft/deberta-v3-base\",\n",
    "                \"albert-large-v2\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        is_deberta = \"deberta\" in model_name.lower()\n",
    "        use_fp16 = torch.cuda.is_available() and (not is_deberta)  # DeBERTa can be unstable in fp16, so we disable it for that model\n",
    "        use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported() and is_deberta\n",
    "\n",
    "        trial.set_user_attr(\"fp16\", bool(use_fp16))\n",
    "        trial.set_user_attr(\"bf16\", bool(use_bf16)) # Faster than normal float32 and no mixed precision instability like fp16 (since same range)\n",
    "\n",
    "        lr          = trial.suggest_float(\"lr\", 5e-6, 5e-5, log=True)\n",
    "        batch_size  = fixed_batch_size\n",
    "        weight_decay= trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "        warmup_ratio= trial.suggest_float(\"warmup_ratio\", 0.0, 0.15)\n",
    "        max_length  = fixed_maxlen   \n",
    "        epochs      = fixed_epochs\n",
    "        grad_accum  = trial.suggest_categorical(\"grad_accum\", [1, 2])\n",
    "        pos_weight_scale = 1.5 # up to 1.5x the base pos weight\n",
    "\n",
    "        tok, ds_train, ds_val, _ = make_tokenized_datasets(model_name, max_length, ds_train_raw, ds_val_raw, ds_dev_raw)\n",
    "\n",
    "        y = np.array(train_split[\"label_bin\"].values, dtype=int)\n",
    "        n_pos = (y == 1).sum()\n",
    "        n_neg = (y == 0).sum()\n",
    "        base_pos_weight = (n_neg / max(n_pos, 1))\n",
    "        pos_weight   = trial.suggest_float(\"pos_weight\", 1, base_pos_weight * pos_weight_scale)\n",
    "        pos_weight_tensor = torch.tensor(pos_weight, dtype=torch.float)\n",
    "        cfg = AutoConfig.from_pretrained(model_name)\n",
    "        cfg.num_labels = 1  # single logit\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, config=cfg)\n",
    "\n",
    "        trial_dir = OUTPUT_DIR / f\"trial_{trial.number:04d}\"\n",
    "        trial_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=str(trial_dir),\n",
    "            seed=SEED,\n",
    "            data_seed=SEED,\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=grad_accum,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=weight_decay,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=1,\n",
    "            fp16=use_fp16,\n",
    "            bf16=use_bf16,\n",
    "            report_to=\"none\",\n",
    "        )\n",
    "\n",
    "        trainer = WeightedBCETrainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=ds_train,\n",
    "            eval_dataset=ds_val,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[\n",
    "                OptunaMedianPruningCallback(trial, monitor=\"eval_f1\"),\n",
    "                EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.003),\n",
    "            ],\n",
    "            pos_weight=pos_weight_tensor,\n",
    "        )\n",
    "\n",
    "        cfg_to_print = {\n",
    "            \"model_name\": model_name,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "            \"max_length\": max_length,\n",
    "            \"epochs\": epochs,\n",
    "            \"grad_accum\": grad_accum,\n",
    "            \"pos_weight\": pos_weight,\n",
    "            \"pos_weight_rel\": pos_weight / base_pos_weight,\n",
    "            \"fp16\": use_fp16,\n",
    "            \"bf16\": use_bf16,\n",
    "        }\n",
    "        pretty_print_dict(f\"Trial {trial.number} config\", cfg_to_print)\n",
    "\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate(ds_val)\n",
    "        pretty_print_dict(f\"Trial {trial.number} metrics\", metrics, sort_keys=False)\n",
    "        return metrics[\"eval_f1\"]\n",
    "\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"OOM: pruning this trial\")\n",
    "        torch.cuda.empty_cache()\n",
    "        raise optuna.TrialPruned()\n",
    "    except KeyboardInterrupt:\n",
    "        try:\n",
    "            trial.set_user_attr(\"interrupted\", True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        raise  # stops the optimize call\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(\"OOM (RuntimeError): pruning this trial\")\n",
    "            torch.cuda.empty_cache()\n",
    "            raise optuna.TrialPruned()\n",
    "        else:\n",
    "            raise\n",
    "    finally:\n",
    "        try:\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e357e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-21 02:15:18,587]\u001b[0m Using an existing study with name 'pcl_task1_binary' instead of creating a new one.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "storage_url = f\"sqlite:///{STUDY_DB}\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    direction=\"maximize\",\n",
    "    storage=storage_url,\n",
    "    load_if_exists=True,\n",
    "    pruner=MedianPruner(n_startup_trials=13, n_warmup_steps=4, n_min_trials=7, interval_steps=1)\n",
    ")\n",
    "\n",
    "stale = mark_stale_running_trials_as_fail(study)\n",
    "if stale:\n",
    "    print(\"Marked stale RUNNING trials as FAIL:\", stale)\n",
    "\n",
    "TARGET_DONE = 50  # total COMPLETE+PRUNED (across sessions)\n",
    "to_run = remaining_trials_to_run(study, TARGET_DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a535426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial state counts: {'RUNNING': 0, 'COMPLETE': 7, 'PRUNED': 0, 'FAIL': 15, 'WAITING': 0}\n",
      "Will run 43 new trials to reach 50 done (COMPLETE+PRUNED).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7115/7115 [00:00<00:00, 18155.28 examples/s]\n",
      "Map: 100%|██████████| 1256/1256 [00:00<00:00, 19106.78 examples/s]\n",
      "Map: 100%|██████████| 2094/2094 [00:00<00:00, 6312.44 examples/s]\n",
      "Loading weights: 100%|██████████| 25/25 [00:00<00:00, 401.47it/s, Materializing param=albert.pooler.weight]                                                             \n",
      "\u001b[1mAlbertForSequenceClassification LOAD REPORT\u001b[0m from: albert-large-v2\n",
      "Key                          | Status     | \n",
      "-----------------------------+------------+-\n",
      "predictions.dense.weight     | UNEXPECTED | \n",
      "predictions.LayerNorm.weight | UNEXPECTED | \n",
      "predictions.bias             | UNEXPECTED | \n",
      "predictions.decoder.bias     | UNEXPECTED | \n",
      "predictions.dense.bias       | UNEXPECTED | \n",
      "predictions.LayerNorm.bias   | UNEXPECTED | \n",
      "classifier.weight            | MISSING    | \n",
      "classifier.bias              | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_2dbfa\">\n",
       "  <caption>Trial 22 config</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbfa_level0_col0\" class=\"col_heading level0 col0\" >key</th>\n",
       "      <th id=\"T_2dbfa_level0_col1\" class=\"col_heading level0 col1\" >value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row0_col0\" class=\"data row0 col0\" >bf16</td>\n",
       "      <td id=\"T_2dbfa_row0_col1\" class=\"data row0 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row1_col0\" class=\"data row1 col0\" >epochs</td>\n",
       "      <td id=\"T_2dbfa_row1_col1\" class=\"data row1 col1\" >12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row2_col0\" class=\"data row2 col0\" >fp16</td>\n",
       "      <td id=\"T_2dbfa_row2_col1\" class=\"data row2 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row3_col0\" class=\"data row3 col0\" >grad_accum</td>\n",
       "      <td id=\"T_2dbfa_row3_col1\" class=\"data row3 col1\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row4_col0\" class=\"data row4 col0\" >lr</td>\n",
       "      <td id=\"T_2dbfa_row4_col1\" class=\"data row4 col1\" >0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row5_col0\" class=\"data row5 col0\" >max_length</td>\n",
       "      <td id=\"T_2dbfa_row5_col1\" class=\"data row5 col1\" >128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row6_col0\" class=\"data row6 col0\" >model_name</td>\n",
       "      <td id=\"T_2dbfa_row6_col1\" class=\"data row6 col1\" >albert-large-v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row7_col0\" class=\"data row7 col0\" >pos_weight</td>\n",
       "      <td id=\"T_2dbfa_row7_col1\" class=\"data row7 col1\" >4.008505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row8_col0\" class=\"data row8 col0\" >pos_weight_scale</td>\n",
       "      <td id=\"T_2dbfa_row8_col1\" class=\"data row8 col1\" >0.417397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row9_col0\" class=\"data row9 col0\" >warmup_ratio</td>\n",
       "      <td id=\"T_2dbfa_row9_col1\" class=\"data row9 col1\" >0.005122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_2dbfa_row10_col0\" class=\"data row10 col0\" >weight_decay</td>\n",
       "      <td id=\"T_2dbfa_row10_col1\" class=\"data row10 col1\" >0.012093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x776b5d77f4c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='116' max='2676' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 116/2676 01:33 < 35:02, 1.22 it/s, Epoch 0.52/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2026-02-21 02:16:57,947]\u001b[0m Trial 22 failed with parameters: {'model_name': 'albert-large-v2', 'lr': 2.6803508238399554e-05, 'weight_decay': 0.01209275925113551, 'warmup_ratio': 0.005121893102694125, 'grad_accum': 2, 'pos_weight': 4.008505329143361} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/joshua_killa/.pyenv/versions/pcl-env/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_296226/357774104.py\", line 96, in objective\n",
      "    trainer.train()\n",
      "  File \"/home/joshua_killa/.pyenv/versions/pcl-env/lib/python3.10/site-packages/transformers/trainer.py\", line 1412, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/joshua_killa/.pyenv/versions/pcl-env/lib/python3.10/site-packages/transformers/trainer.py\", line 1747, in _inner_training_loop\n",
      "    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2026-02-21 02:16:57,949]\u001b[0m Trial 22 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted. Rerun this cell to continue toward the target.\n",
      "Trial state counts: {'RUNNING': 0, 'COMPLETE': 7, 'PRUNED': 0, 'FAIL': 16, 'WAITING': 0}\n",
      "Best f1: 0.5566037735849056\n",
      "Best params: {'model_name': 'albert-large-v2', 'lr': 7.295763634857713e-06, 'weight_decay': 0.07317734691943754, 'warmup_ratio': 0.10647176648296738, 'grad_accum': 2, 'pos_weight': 3.2832558029026764}\n"
     ]
    }
   ],
   "source": [
    "print(\"Trial state counts:\", done_counts(study))\n",
    "print(f\"Will run {to_run} new trials to reach {TARGET_DONE} done (COMPLETE+PRUNED).\")\n",
    "\n",
    "try:\n",
    "    if to_run > 0:\n",
    "        study.optimize(objective, n_trials=to_run, gc_after_trial=True)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted. Rerun this cell to continue toward the target.\")\n",
    "\n",
    "print(\"Trial state counts:\", done_counts(study))\n",
    "print(\"Best f1:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c42f4",
   "metadata": {},
   "source": [
    "### Grid search database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ea6384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>state</th>\n",
       "      <th>value</th>\n",
       "      <th>params_grad_accum</th>\n",
       "      <th>params_lr</th>\n",
       "      <th>params_model_name</th>\n",
       "      <th>params_pos_weight</th>\n",
       "      <th>params_pos_weight_scale</th>\n",
       "      <th>params_warmup_ratio</th>\n",
       "      <th>params_weight_decay</th>\n",
       "      <th>user_attrs_bf16</th>\n",
       "      <th>user_attrs_fp16</th>\n",
       "      <th>user_attrs_interrupted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.556604</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>3.283256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106472</td>\n",
       "      <td>0.073177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.937763</td>\n",
       "      <td>0.018057</td>\n",
       "      <td>0.089044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.335515</td>\n",
       "      <td>0.141454</td>\n",
       "      <td>0.054436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.263415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>11.248359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.110859</td>\n",
       "      <td>0.041321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.261574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.195678</td>\n",
       "      <td>0.144863</td>\n",
       "      <td>0.090752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.173343</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.078698</td>\n",
       "      <td>0.101572</td>\n",
       "      <td>0.093909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.172965</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.852154</td>\n",
       "      <td>0.085964</td>\n",
       "      <td>0.020242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     state     value  params_grad_accum  params_lr  \\\n",
       "10      10  COMPLETE  0.556604                2.0   0.000007   \n",
       "5        5  COMPLETE  0.470588                1.0   0.000005   \n",
       "0        0  COMPLETE  0.400000                1.0   0.000013   \n",
       "9        9  COMPLETE  0.263415                1.0   0.000024   \n",
       "1        1  COMPLETE  0.261574                1.0   0.000043   \n",
       "2        2  COMPLETE  0.173343                2.0   0.000045   \n",
       "4        4  COMPLETE  0.172965                2.0   0.000031   \n",
       "\n",
       "   params_model_name  params_pos_weight  params_pos_weight_scale  \\\n",
       "10   albert-large-v2           3.283256                      NaN   \n",
       "5    albert-large-v2                NaN                 0.937763   \n",
       "0    albert-large-v2                NaN                 1.335515   \n",
       "9    albert-large-v2          11.248359                      NaN   \n",
       "1    albert-large-v2                NaN                 1.195678   \n",
       "2    albert-large-v2                NaN                 1.078698   \n",
       "4    albert-large-v2                NaN                 0.852154   \n",
       "\n",
       "    params_warmup_ratio  params_weight_decay user_attrs_bf16 user_attrs_fp16  \\\n",
       "10             0.106472             0.073177             NaN             NaN   \n",
       "5              0.018057             0.089044             NaN             NaN   \n",
       "0              0.141454             0.054436             NaN             NaN   \n",
       "9              0.110859             0.041321             NaN             NaN   \n",
       "1              0.144863             0.090752             NaN             NaN   \n",
       "2              0.101572             0.093909             NaN             NaN   \n",
       "4              0.085964             0.020242             NaN             NaN   \n",
       "\n",
       "   user_attrs_interrupted  \n",
       "10                    NaN  \n",
       "5                     NaN  \n",
       "0                     NaN  \n",
       "9                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "4                     NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_so_far_df(study, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963ac69",
   "metadata": {},
   "source": [
    "## Train best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = study.best_params\n",
    "best_model = best[\"model_name\"]\n",
    "\n",
    "is_deberta = \"deberta\" in best_model.lower()\n",
    "use_fp16 = torch.cuda.is_available() and (not is_deberta)  # DeBERTa can be unstable in fp16, so we disable it for that model\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported() and is_deberta\n",
    "\n",
    "# rebuild datasets: train on (train+val), evaluate on dev\n",
    "full_train = pd.concat([train_split, val_split], ignore_index=True)\n",
    "ds_full_train_raw = Dataset.from_pandas(full_train.reset_index(drop=True))\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(best_model, use_fast=True)\n",
    "\n",
    "def tok_fn(batch):\n",
    "    texts = [str(x) if x is not None else \"\" for x in batch[\"text\"]]\n",
    "    tokenized = tok(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=fixed_maxlen,\n",
    "    )\n",
    "    if \"label_bin\" in batch:\n",
    "        tokenized[\"labels\"] = [float(x) for x in batch[\"label_bin\"]]\n",
    "    return dict(tokenized)\n",
    "\n",
    "# Tokenize datasets & truncate/pad them to max length\n",
    "ds_full_train = ds_full_train_raw.map(tok_fn, batched=True)\n",
    "ds_dev = ds_dev_raw.map(tok_fn, batched=True)\n",
    "\n",
    "# Only rename if needed\n",
    "if \"labels\" not in ds_full_train.column_names and \"label_bin\" in ds_full_train.column_names:\n",
    "    ds_full_train = ds_full_train.rename_column(\"label_bin\", \"labels\")\n",
    "if \"labels\" not in ds_dev.column_names and \"label_bin\" in ds_dev.column_names:\n",
    "    ds_dev = ds_dev.rename_column(\"label_bin\", \"labels\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "ds_full_train = ds_full_train.remove_columns([c for c in ds_full_train.column_names if c not in [\"input_ids\", \"attention_mask\", \"labels\"]])\n",
    "ds_dev = ds_dev.remove_columns([c for c in ds_dev.column_names if c not in [\"input_ids\", \"attention_mask\", \"labels\"]])\n",
    "\n",
    "pos_weight_tensor = torch.tensor(best[\"pos_weight\"], dtype=torch.float)\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(best_model)\n",
    "cfg.num_labels = 1\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_model, config=cfg)\n",
    "\n",
    "final_dir = OUTPUT_DIR / \"best_final_model\"\n",
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(final_dir),\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    learning_rate=best[\"lr\"],\n",
    "    per_device_train_batch_size=fixed_batch_size,\n",
    "    per_device_eval_batch_size=fixed_batch_size,\n",
    "    gradient_accumulation_steps=best[\"grad_accum\"],\n",
    "    num_train_epochs=fixed_epochs,\n",
    "    weight_decay=best[\"weight_decay\"],\n",
    "    warmup_ratio=best[\"warmup_ratio\"],\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,\n",
    "    fp16=use_fp16,\n",
    "    bf16=use_bf16,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = WeightedBCETrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_full_train,\n",
    "    eval_dataset=ds_dev,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    "    pos_weight=pos_weight_tensor,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "dev_metrics = trainer.evaluate(ds_dev)\n",
    "dev_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
